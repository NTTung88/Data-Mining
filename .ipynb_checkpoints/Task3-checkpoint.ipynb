{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeNTCKnKnN6R"
   },
   "source": [
    "# I.Import Library\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AN8rgbmrdKJP",
    "outputId": "22bae130-3fd4-4220-956d-a31a1cbb6bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from keras==2.2.5) (1.0.8)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.5) (1.15.0)\n",
      "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.22.2.post1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.0.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%tensorflow_version 1.x\n",
    "!pip install keras==2.2.5\n",
    "!pip install pyvi\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import os, pickle, re, keras, sklearn, string\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import gensim, operator, json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import *\n",
    "import keras.backend as K\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers\n",
    "from keras import optimizers\n",
    "from keras.engine.topology import Layer\n",
    "from keras import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqqjA5rmnXW9"
   },
   "source": [
    "# II.Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_QyBjmaFumz",
    "outputId": "1d449deb-0beb-41a7-a1bb-976766fa1268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-05-16 14:59:47--  https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.model.bin\n",
      "Resolving thiaisotajppub.s3-ap-northeast-1.amazonaws.com (thiaisotajppub.s3-ap-northeast-1.amazonaws.com)... 52.219.8.78\n",
      "Connecting to thiaisotajppub.s3-ap-northeast-1.amazonaws.com (thiaisotajppub.s3-ap-northeast-1.amazonaws.com)|52.219.8.78|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 708212586 (675M) [application/macbinary]\n",
      "Saving to: ‘baomoi.model.bin.2’\n",
      "\n",
      "baomoi.model.bin.2  100%[===================>] 675.40M  19.8MB/s    in 36s     \n",
      "\n",
      "2021-05-16 15:00:24 (18.9 MB/s) - ‘baomoi.model.bin.2’ saved [708212586/708212586]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6I-G2B7CwyA",
    "outputId": "84410af8-032f-4cda-fa32-6406e235d740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zipfile.ZipFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"UIT-VSMEC.zip\",\"r\") as zf:\n",
    "    zf.extractall()\n",
    "print(zf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Rp1-Qa_81FEY"
   },
   "outputs": [],
   "source": [
    "path_train ='comments_Task1.csv'\n",
    "path_valid ='data/valid_nor_811.csv'\n",
    "path_test ='data/test_nor_811.csv'\n",
    "path_stopword = 'data/stopwords.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3JUFIaAndQo"
   },
   "source": [
    "# III.Word2vec using baomoi.model.bin\n",
    "\n",
    "*   Function reading pretrain word embedding library.\n",
    "*   The word embedding pretrain has been trained in new news, 300-way news\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Skg7lUZ9LYAn",
    "outputId": "59a6660a-af6c-4419-ba70-8563754acef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding:  400\n",
      "[-0.78774583 -0.22327825 -0.6274532  -2.7228408  -2.2186291   0.38002455\n",
      "  3.8660462   0.9853684  -1.4683082  -1.7013292  -0.5839958  -0.14467287\n",
      "  3.600142    3.381808   -0.02930526  3.0047843  -0.2006207  -1.0937127\n",
      "  1.7360235   2.3691583  -0.71597415  3.319453    0.2824182  -3.0814204\n",
      "  2.6810844  -0.810977    1.5186927  -2.10329     1.3271075  -1.3646411\n",
      " -0.11144319 -4.6505136  -1.7251624  -2.31126     1.583203   -0.8746506\n",
      " -2.6937015  -1.7733976   0.557898   -1.7562917   1.3282276  -0.3805479\n",
      " -1.3979301  -0.1536707  -1.1909302   1.3283668   0.22275637 -2.7959821\n",
      " -5.188217   -0.6404673   0.0164395   0.67177856 -1.4948794   0.21867418\n",
      " -1.4103376   0.99262404  2.2180524  -0.4881204   3.0988753  -0.31382522\n",
      "  1.3226501   0.21269594 -1.6409203   1.7758838   2.3379912  -2.4666297\n",
      " -0.599687    0.551105   -1.3755493  -1.4293027  -2.6366289   0.40759587\n",
      " -0.77850854 -0.6169452  -0.84525913  0.02801617  2.1296268   0.13715844\n",
      " -1.1562283  -2.1226277  -0.1346792   0.88932824 -1.5711976   0.36148685\n",
      "  2.2572796  -2.0762215   1.736077   -0.3133224   0.48849696  2.1262195\n",
      " -2.3417432   0.7264937   1.3197432  -1.0578146   1.9603167  -1.957219\n",
      " -0.47556064 -3.2944543  -1.540249    1.6060241   0.02990843 -1.0645736\n",
      " -0.5550473   1.589397   -0.5811684  -1.2199221  -0.9025384  -1.2436662\n",
      "  0.50163126  0.11698119  0.8760743   0.8978141  -1.8893797  -0.1424527\n",
      " -3.0423136  -0.88489795  0.49000955 -3.4689097  -1.8564429  -0.66697997\n",
      "  5.3912683  -2.092744   -0.5973023  -0.7118058  -1.0953093   0.4417508\n",
      "  2.440871    1.1271865   1.4602836  -2.714987   -1.3927895  -0.16143057\n",
      "  0.07596377 -2.0885456  -1.0929846  -1.1670731   0.7352281  -1.0726835\n",
      "  0.4963534  -0.78458273  2.3078787  -3.5055773   0.9567256  -0.5207236\n",
      "  0.36697528  0.8511779   0.878965    1.3028007   0.04724613  0.9892602\n",
      " -0.8373782   0.27926713  2.268885   -0.11917569  0.8163163   0.3869213\n",
      "  0.20561185 -2.2969527  -1.6468542  -3.9922614  -0.96281457  3.2537632\n",
      "  0.48358652 -0.6078726   3.2632709   0.11489751 -2.6600893   0.92677915\n",
      " -0.528953    3.4760187   2.31958     2.23189     2.2253554  -1.8307585\n",
      " -1.7324418  -1.2364737  -4.273679    0.9341229   0.59669524 -0.03376843\n",
      " -2.971719    1.9712305  -0.549242    0.4829846   1.4618144  -1.3703161\n",
      " -1.1212839   0.4291749   1.4675773  -0.67144257 -0.49444234  1.7652586\n",
      "  1.7143794   0.54265493  2.1978571   3.2426474  -2.7528286  -2.2640996\n",
      "  0.09805597  1.2702079   1.156494   -1.1671641  -2.3361897   1.2424865\n",
      " -2.1413488  -0.22989413 -2.7570245  -1.2689328  -0.11422111  0.340871\n",
      " -0.72356385 -3.100242   -0.2113436   0.08352826  3.0843058   0.2549431\n",
      "  3.6576512  -0.71284246 -1.8232595  -1.0566906  -0.7372802   0.18872899\n",
      "  0.11927979  3.0378866   0.7687284   0.7458194   3.392024   -0.10601766\n",
      "  1.7550762  -3.9328496   0.5543825   2.4240685   2.4877627  -1.8583341\n",
      " -2.7361338   0.9327119   1.4136555   0.6736002  -0.56006515 -0.17299697\n",
      "  2.3964696   1.4890865   0.47563386 -1.7579868  -3.2750478  -2.711356\n",
      "  1.1631078   0.5226146  -0.77252626  2.0378802  -2.1662908   1.1695647\n",
      "  1.0302314   1.2815226   1.8774925   1.0482382  -2.829525   -1.3818443\n",
      "  1.0274167  -0.61302423  0.24060939 -2.8208141   1.2254591  -1.9544963\n",
      "  1.7342434  -0.9264713   0.39906958  1.4076114   0.68284744  2.4939642\n",
      "  2.1315014   0.20849855 -1.4851028   4.6376705   2.2327776   2.2943447\n",
      " -1.253777   -2.385657   -2.5678577  -1.7067193   2.387362   -1.3572613\n",
      " -4.5016227   0.7827232  -0.44352373  0.3998332  -0.5178318   0.6794015\n",
      "  3.7974224  -0.774167   -1.1981938   0.3985697  -1.8760519   0.1238703\n",
      "  0.05213618 -1.1321199  -2.8599005  -1.7278007   2.1998515  -2.5468414\n",
      "  0.9428754   0.992005   -2.7554674  -1.364683   -0.8704408  -1.1697435\n",
      "  1.880865    1.9564949   3.1174672   0.14133504 -2.2360458  -1.173718\n",
      "  1.3261789  -2.2110426   0.589773    3.4267988   3.2046275  -0.91721445\n",
      "  0.7951813  -2.3174386  -0.8497346   1.6120318  -0.40876418  0.7933062\n",
      " -0.8393237  -0.41007656  2.8945262   0.993892   -1.7588191  -4.925732\n",
      " -2.4419074   3.0766954  -1.9000514  -2.0604458   3.4133394  -2.0102491\n",
      "  2.046963    0.71078193  2.2965722   1.72548     1.2269657  -2.2357993\n",
      "  0.50008225  4.8475957  -1.0541344   2.308317   -1.1630418   1.1258802\n",
      " -1.2478187   1.0942221   0.92810786 -1.5838045  -2.0654778  -0.28107494\n",
      "  2.4073355   1.3492072   0.48608387 -0.4716219   2.1616933  -1.3125483\n",
      "  1.5123384  -4.5521007  -1.8622614   1.088746    1.6043898  -4.609651\n",
      " -0.4500263   1.9802842   1.9102592  -0.47483805 -4.2590923   1.1523023\n",
      "  0.9832213   1.9767743   3.2593958   0.41252086  0.37052628  0.5532108\n",
      " -0.44213554  1.3767333  -0.8914752  -4.444391   -1.9290444  -1.6292545\n",
      " -2.6812217  -1.0284953   0.0313996   4.0027394   0.6476944  -0.02785059\n",
      " -2.4854484  -0.82289666  1.3539648   3.0980484  -1.3214976  -1.5370079\n",
      "  0.96148336  0.1992502   3.5529153   2.4300497 ]\n"
     ]
    }
   ],
   "source": [
    "path_embedding= 'baomoi.model.bin'\n",
    "\n",
    "import io\n",
    "from gensim.models import KeyedVectors\n",
    "word_embedding = KeyedVectors.load_word2vec_format(path_embedding, binary=True)\n",
    "EMBEDDING_DIM = word_embedding['yêu'].shape[0]\n",
    "print(\"Embedding: \",EMBEDDING_DIM)\n",
    "print(word_embedding['yêu'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo7k112MoVAQ"
   },
   "source": [
    "# IV. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "MWTniSb1o0d9"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    token = ViTokenizer.tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VCchYWSMo9y5"
   },
   "outputs": [],
   "source": [
    "def deleteIcon(text):\n",
    "    text = text.lower()\n",
    "    s = ''\n",
    "    pattern = r\"[a-zA-ZaăâbcdđeêghiklmnoôơpqrstuưvxyàằầbcdđèềghìklmnòồờpqrstùừvxỳáắấbcdđéếghíklmnóốớpqrstúứvxýảẳẩbcdđẻểghỉklmnỏổởpqrstủửvxỷạặậbcdđẹệghịklmnọộợpqrstụựvxỵãẵẫbcdđẽễghĩklmnõỗỡpqrstũữvxỹAĂÂBCDĐEÊGHIKLMNOÔƠPQRSTUƯVXYÀẰẦBCDĐÈỀGHÌKLMNÒỒỜPQRSTÙỪVXỲÁẮẤBCDĐÉẾGHÍKLMNÓỐỚPQRSTÚỨVXÝẠẶẬBCDĐẸỆGHỊKLMNỌỘỢPQRSTỤỰVXỴẢẲẨBCDĐẺỂGHỈKLMNỎỔỞPQRSTỦỬVXỶÃẴẪBCDĐẼỄGHĨKLMNÕỖỠPQRSTŨỮVXỸ,._]\"\n",
    "    for char in text:\n",
    "        if char !=' ':\n",
    "            if len(re.findall(pattern, char)) != 0:\n",
    "                s+=char\n",
    "            elif char == '_':\n",
    "                s+=char\n",
    "        else:\n",
    "            s+=char\n",
    "    s = re.sub('\\\\s+',' ',s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "L1_TFsm9WdA0"
   },
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    doc = tokenizer(doc)\n",
    "    for punc in string.punctuation:\n",
    "        if punc != \"_\":\n",
    "            doc = doc.replace(punc,' ')\n",
    "    doc = deleteIcon(doc) \n",
    "    doc = re.sub(r\"[0-9]+\", \" num \", doc)\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub('\\\\s+',' ',doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hPeiHFV2vRMY"
   },
   "outputs": [],
   "source": [
    "# from underthesea import word_tokenize\n",
    "def pre_process(questions):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    questions_stop = [[t for t in tokens if (t not in stop_words) and (3 < len(t.strip()) < 15)]\n",
    "                      for tokens in questions_tokens]\n",
    "    questions_stop = pd.Series(questions_stop)\n",
    "    return questions_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmCkYJpypwlS"
   },
   "source": [
    "# V.Train/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Usy-4ygPWdA2"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(path_train)\n",
    "valid_data = pd.read_csv(path_valid)\n",
    "test_data = pd.read_csv(path_test)\n",
    "\n",
    "X_train = train_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_train = train_data[\"Emotion\"]\n",
    "\n",
    "X_val = valid_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_val = valid_data[\"Emotion\"]\n",
    "\n",
    "X_test = test_data[\"Sentence\"].apply(lambda x : clean_doc(x))\n",
    "y_test = test_data[\"Emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GnYcGTbL0jnu",
    "outputId": "3b37b0a0-b76a-4964-e02b-5e4f5b2570b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 421\n",
      "686 686\n",
      "693 693\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(y_train))\n",
    "print(len(X_val),len(y_val))\n",
    "print(len(X_test),len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "fyIQeZi-WdA4"
   },
   "outputs": [],
   "source": [
    "classes = ['Anger','Disgust','Enjoyment','Fear','Other','Sadness','Surprise']\n",
    "def to_category_vector(label):\n",
    "    vector = np.zeros(len(classes)).astype(np.float64)\n",
    "    index = classes.index(label)\n",
    "    vector[index] = 1.0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fqYyTPmLWdA_",
    "outputId": "e36bef4a-e053-43d4-d678-5b77098cf24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anger', 'Disgust', 'Enjoyment', 'Fear', 'Other', 'Sadness', 'Surprise']\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n",
      "Enjoyment\n"
     ]
    }
   ],
   "source": [
    "y_train_encode = []\n",
    "for label in y_train:\n",
    "    y_train_encode.append(to_category_vector(label))\n",
    "\n",
    "y_val_encode = []\n",
    "for label in y_val:\n",
    "    y_val_encode.append(to_category_vector(label))\n",
    "\n",
    "print(classes)\n",
    "print(y_train_encode[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK6nyuqOq171"
   },
   "source": [
    "### LSTM\n",
    "\n",
    "\n",
    "*   All the words in the X_train set will form a dictionary\n",
    "*   Each vector of the input word, it will turn into a vector with a fixed number of dimensions and each vocabulary will be replaced by its index in the dictionary\n",
    "* Number of vector dimensions per input we will take the longest sentence which is the direction of the vector and the shorter arcs will automatically add the value 0 after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0EB_NYPlWdBC",
    "outputId": "35873007-040a-49e1-f002-6ddb7e27db8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence length value:  268\n",
      "input_vocab_size: 1104\n"
     ]
    }
   ],
   "source": [
    "xLengths = [len(x.split(' ')) for x in X_train]\n",
    "h = sorted(xLengths)  \n",
    "maxLength =h[len(h)-1]\n",
    "print(\"The longest sentence length value: \",maxLength)\n",
    "input_tokenizer = Tokenizer(filters=\"\",oov_token=\"UNK\")\n",
    "input_tokenizer.fit_on_texts(X_train)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "word_index = input_tokenizer.word_index\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "X_train_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_train), maxlen=maxLength,padding=\"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KxIekrnfrfBg",
    "outputId": "33272dde-46a4-45db-9fd5-fec80b6d456a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String :  dân_chơi xóm giàu vì bạn sang vì vợ ala_ela đây là rap_việt\n",
      "Encode :  [105  89  38  10  17  33  10  34 189  25   4 132   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input String : \", X_train[0])\n",
    "print(\"Encode : \",X_train_encode[0])\n",
    "\n",
    "X_val_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_val), maxlen=maxLength,padding=\"post\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Td-ML6bqWdBE"
   },
   "outputs": [],
   "source": [
    "def generate_embedding(word_index, model_embedding,EMBEDDING_DIM):\n",
    "    count6 = 0\n",
    "    countNot6 = 0\n",
    "    embedding_matrix = np.asarray([np.random.uniform(-0.01,0.01,EMBEDDING_DIM) for _ in range((len(word_index) + 1))])\n",
    "    list_oov = []\n",
    "    word_is_trained = []\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = model_embedding[word]\n",
    "            word_is_trained.append(word)\n",
    "        except:\n",
    "            continue\n",
    "        if embedding_vector is not None:\n",
    "            count6 +=1\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    print('Number of words in pre-train embedding: ' + str(count6))\n",
    "    print('Number of words not in pre-train embedding: ' + str(countNot6))\n",
    "    return embedding_matrix,word_is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRIghLqMBuhV",
    "outputId": "feb5178c-1558-4b2c-8987-af1f5af4ea80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in pre-train embedding: 987\n",
      "Number of words not in pre-train embedding: 0\n",
      "['hay', 'bài', 'là', 'nghe', 'rap', 'của', 'mck', 'có', 'vì', 'vẫn', 'em', 'nhất', 'thì', 'cũng', 'quá', 'bạn', 'luôn', 'ko', 'việt', 'kim', 'mình', 'anh', 'mà', 'đây', 'thang', 'gì', 'và', 'người', 'bắc', 'nhưng', 'k', 'sang', 'vợ', 'như', 'nào', 'sao', 'giàu', 'đỉnh', 'rồi', 'các', 'ai', 'hơn', 'thấy', 'để', 'ạ', 'làm', 'cho', 'đã', 'binz', 'được', 'về', 'karik', 'cái', 'này', 'nó', 'xem', 'con', 'nhiều', 'đến', 'thích', 'mấy', 'nhạc', 'đó', 'từ', 'tr', 'ấy', 'vào', 'đúng', 'cứ', 'đầu', 'view', 'bác', 'ricky', 'hát', 'chỉ', 'với', 'đc', 'tui', 'r', 'mọi', 'lại', 'cả', 'nón', 'tiền', 'xóm', 'bắc_kim_thang', 'khi', 'nổi', 'ông', 'biết', 'bị', 'à', 'không', 'giọng', 'đang', 'trên', 'triệu', 'thuộc', 'chất', 'thêm', 'dân_chơi', 'sau', 'đầu_tiên', 'nay', 'da', 'ráp', 'hết', 'phải', 'tất_cả', 'vl', 'nữa', 'xong', 'thiếu', 'vs', 'yêu', 'mới', 'trong', 'dùng', 'kiểu', 'x', 'rất', 'tôi', 'nói', 'hiểu', 'qua', 'đi', 'chả', 'rap_việt', 'năm', 'câu', 'da_gà', 'á', 'chưa', 'đoạn', 'đều', 'những', 'lần', 'giờ', 'còn', 'lên', 'sợ', 'e', 'bé', 'lắm', 'số', 'chán', 'một', 'j', 'video', 'nếu', 'nên', 'star', 'thứ', 'vô', 'nhỉ', 'chứ', 'c', 'best', 'wowy', 'thi', 'vãi', 'vừa', 'ơi', 'ngừng', 'vàng', 'ca', 'bật', 'chục', 'is', 'chắc', 'tai', 'loại', 'ý', 'đâu', 'chọn', 'chóp', 'ra', 'may', 'khá', 'sp', 'm', 'thử', 'cần', 'nợ', 't', 'độc_nhất', 'cùng', 'nhứt', 'phiêu', 'theo', 'fan', 'hoặc', 'giống', 'ad', 'vd', 'nghiện', 'dân', 'giúp', 'mây', 'trước', 'suboi', 'justatee', 'v', 'cuối', 'a', 'co', 'nhảy', 'trời', 'đù', 'tháng', 'ngày', 'ae', 'ma_mị', 'like', 'thật_sự', 'gần', 'love', 'nha', 'cách', 'you', 'chú', 'loắt_choắt', 'kì', 'mn', 'trắng', 'bên', 'team', 'phê', 'bây_giờ', 'chết', 'ta', 'the_best', 'quán_quân', 'zalo', 'tao', 'chăm_chỉ', 'xin', 'gỡ', 'vài', 'tới', 'mùa', 'sẽ', 'lão', 'men', 'nhị', 'đọc', 'ghê', 'đứa', 'hlv', 'tee', 'đặc_biệt', 'beat', 'tổng_hợp', 'mở', 'nge', 'mạnh', 'h', 'ý_nghĩa', 'thôi', 'chim_sẻ', 'dâu_tây', 'khúc', 'the', 'đăng', 'mỗi', 'chúc', 'tham_gia', 'học', 'bởi', 'thay', 'trái_đất', 'tuổi', 'đội', 'chơi', 'nhở', 'nhịp', 'cháu', 'la', 'kĩ', 'đông', 'tay', 'so', 'mk', 'ít', 'mie', 'má', 'xưa', 'ngáo', 'rùi', 'đeo', 'hội', 'lun', 'vậy', 'great', 'n', 'muốn', 'hôm_nay', 'thế_giới', 'tầm', 'cao', 'kinh_dị', 'amazing', 'noi', 'ik', 'ngu', 'rạp', 'sai', 'tý', 'good', 'dế', 'choắt', 'chị', 'p', 'tên', 'quẩy', 'su', 'ala', 'ela', 'quỳ', 'có_thể', 'clip', 'go', 'sớm', 'thề', 'quăng', 'chi', 'zing', 'bản', 'xem_lại', 'l', 'dưỡng', 'sử_dụng', 'bảo', 'nghĩ', 'tự_nhiên', 'đen', 'tư_vấn', 'speed', 'liền', 'ng', 'riêng', 'ok', 'dinh', 'quảng_cáo', 'xinh_tươi', 've', 'tuyệt_vời', 'g', 'lai', 'ban', 'tâm_linh', 'con_gái', 'gi', 'toàn', 'ở', 'ver', 'hieu', 'ơ', 'ah', 'thich', 'bai', 'cua', 'bat', 'bình_thường', 'vi', 'chỗ', 'kia', 'thật', 'vòng_tay', 'nhac', 'thua', 'to', 'nos', 'cuộc_sống', 'tìm', 'cuộc_đời', 'woow', 'đề', 'ngờ', 'mua', 'nựa', 'lúc', 'ăn', 'đéo', 'đấm', 'cữa', 'mang', 'dấu_ấn', 'cực_kì', 'đậm_nét', 'cho_dù', 'nhắc', 'nhớ', 'ngay', 'tình_yêu', 'bận_bịu', 'mở_đầu', 'chất_ngất', 'giám_khảo', 'nhấn', 'nút', 'ấp_ủ', 'ms', 'gặp', 'feat', 'nỗi', 'dịch', 'chời', 'info', 'áo', 'văn', 'ls', 'nhờ', 'chung_kết', 'win', 'mẹ', 'tối', 'định', 'đón', 'giao_thừa', 'bgk', 'tự_do', 'bung', 'nấc', 'nhường', 'spotlight', 'thí_sinh', 'chắc_chắn', 'mong', 'tour', 'up', 'hit', 'điều', 'mong_chờ', 'time', 'stop', 'cười', 'màng', 'said', 'chúng_mày', 'anh_em', 'thầy', 'sau_này', 'giai_điệu', 'thiểu_năng', 'chê', 'u_mê', 'anh_tú', 'yêu_dấu', 'giải_trí', 'khỏi', 'căng_thẳng', 'áp_lực', 'mai', 'tinh_thần', 'thoải_mái', 'học_kỳ', 'thành_công', 'quá_khứ', 'kịp', 'tham_dự', 'tương_lai', 'nhất_định', 'mode', 'travis', 'scott', 'xoa', 'kk', 'mê', 'mai_mốt', 'con_cò', 'coi_chừng', 'ma_da', 'run', 'cấp', 'hóng', 'hai', 'rồng_rắn', 'thánh', 'lunn', 'ám_ảnh', 'gia_huy', 'dancer', 'biểu_diễn', 'sân', 's', 'xuống', 'khán_giả', 'tuyệt_phẩm', 'chờ_đợi', 'quay', 'bay', 'cô_dâu', 'phát_sóng', 'nắm', 'trùm', 'tiết_mục', 'học_trò', 'ổng', 'squad', 'phết', 'mùi', 'big_bang', 'sân_khấu', 'bớt', 'nhột', 'câu_đầu', 'khóc_thét', 'ha_ha_ha', 'xanh', 'mặt', 'bum', 'cha', 'bumbum', 'lạnh', 'sẵn_lòng', 'sưởi', 'ấm', 'tóc', 'dài', 'keo', 'vuốt', 'goku', 'tại_sao', 'deep', 'tuy', 'idol', 'giời', 'cài', 'qc', 'thoy', 'dj', 'kìa', 'hồi', 'lẩm_bẩm', 'suốt', 'max', 'king', 'qeen', 'mô', 'xịn', 'nhóm', 'where', 'hông', 'ma', 'nước_cất', 'freestyle', 'nhục', 'nhỡ', 'tem', 'biz', 'b', 'phone', 'model', 'nhanh', 'đỉnh_cao', 'year', 'hiếm', 'lịch_sử', 'vch', 'màn', 'trình_diễn', 'nhìn', 'đam_mê', 'faceboook', 'ôi', 'bỏ_rơi', 'đưa', 'tuổi_thơ', 'y_hệt', 'hết_hồn', 'bình', 'gold', 'côn_đồ', 'đò', 'lỗi', 'chới', 'kéo_dài', 'hỏi', 'đời', 'ham_muốn', 'vơi', 'tar', 'lời', 'lẫn', 'kết', 'justa', 'gd', 'dc', 'hem', 'kkk', 'xinh', 'loạt', 'vid', 'trăm_ngàn', 'đ', 'té', 'ghế', 'đậm', 'cụ', 'quay_phim', 'hiện', 'chữ', 'màn_hình', 'bít', 'auto', 'viral', 'thang_cuốn', 'chết_đứng', 'sống', 'hổng', 'xếp', 'hạnh', 'nhận', 'dễ', 'dành', 'let', 'tag', 'cấm', 'thả', 'thính', 'lung_tung', 'nhá', 'bấm', 'từng', 'diss', 'lăng', 'ld', 'linh', 'tắc', 'thở', 'cmn', 'đẳng_cấp', 'đáng_lẽ', 'thik', 'o', 'gut', 'chop', 'i', 'hiệu_ứng', 'đau', 'đông_lạnh', 'hợp_lý', 'gọi', 'vị_trí', 'dù', 'lẻ', 'gom', 'kênh', 'trên_dưới', 'ấn_tượng', 'dạo', 'sàn', 'hợp', 'van', 'diem', 'danh', 'ô_hay', 'thế', 'vn', 'giộng', 'đi_vào', 'zậy', 'ánh', 'ảm', 'ngăm', 'khoảng', 'thời', 'guan', 'tự_ti', 'than_vãn', 'hoài', 'sữa', 'một_số', 'phù_hợp', 'sv', '_b', 'thuốc_nam', 'oke', 'ảnh_hưởng', 'nguyên_liệu', 'một_vài', 'mụn', 'nhỏ', 'lỗ', 'chân_lông', 'khít', 'kha_khá', 'ngưng', 'thời_gian', 'nhiệt_tình', 'khách', 'độ', 'liên_hệ', 'sđt', 'kiếm', 'tập', 'no', 'bth', 'lố', 'thần', 'jesus', 'queen_latifah', 'vk', 'đẹp', 'nhức', 'nách', 'huấn_luyện_viên', 'cất', 'hot', 'đấy', 'để_ý', 'trừ', 'all', 'có_mặt', 'phút', 'mi', 'vịnh', 'chim', 'đánh', 'oi', 'lớp', 'alo', 'đêm', 'đa_số', 'đẻ', 'bọn', 'chửi', 'thằng', 'moi', 'dau', 'nhug', 'mot', 'màu', 'hát_bắc', 'in_ấn', 'biệt', 'day', 'viet', 'that', 'phiêu_lưu', 'ký', 'cu', 'nhai', 'quai', 'z', 'bao', 'nhieu', 'cai', 'dung', 'giong', 'nhau', 'roi', 'thu', 'vii', 'nhí_nhố', 'boy', 'viết', 'trực_tiếp', 'rõ', 'sịt', 'gút', 'bản_thân', 'lách', 'tin', 'nhắn', 'hỏi_han', 'quan_tâm', 'yêu_thương', 'cx', 'cute', 'cool', 'lp', 'chưởng', 'con_trai', 'muộn', 'nhề', 'bt', 'bj', 'gióng', 'đôi', 'an_giang', 'hói', 'hận', 'het', 'liên_quan', 'grap', 'minh', 'nguoi', 'ducky', 'miền', 'nam', 'dép', 'balo', 'hs', 'aka', 'gà', 'điên', 'ném', 'mũ', 'như_điên', 'bó_tay', 'toi', 'hốt', 'youtube', 'chiếu', 'hình_như', 'vịt', 'giau', 'vo', 'vô_nghĩa', 'lào', 'tuyệt', 'very', 'tuyển_tập', 'lượt', 'ccc', 'oh', 'sâu_lắng', 'tình_cảm', 'kêu', 'chợ', 'bài_hát', 'giá', 'người_yêu', 'du_lịch', 'yuno', 'mấp_mé', 'liều_mạng', 'lôi', 'lọt', 'trôi', 'tuột', 'định_mệnh', 'ủa', 'kh', 'á_quân', 'ố', 'du', 'nac', 'elon_musk', 'toc', 'do', 'chiếm', 'kinh', 'thực_sự', 'tiếc', 'cá_nhân', 'bray', 'đầu_tư', 'vốn', 'thấp', 'lợi_nhuận', 'hàng', 'ib', 'luôn_luôn', 'if', 'want', 'god', 'forgive', 'dont', 'commet', 'here', 'just', 'church', 'and', 'pray', 'con_mẹ', 'tạt', 'an', 'buoi', 'lũ', 'chat', 'que', 'dios', 'buck', 'foy', 'vân', 'bằng', 'uno', 'huyền_thoại', 'niệm_phật', 'nam_mô', 'bổn_sư', 'thích_ca', 'mâu_ni_phật', 'ác_nghiệp', 'muôn_thuở', 'tham', 'sân_si', 'thân', 'miệng', 'phát_sinh', 'thành_tâm', 'sám_hối', 'hãy', 'mở_đường', 'lối', 'học_tập', 'lao_động', 'lo', 'lao', 'đồng', 'việc', 'thiện', 'phật_pháp', 'giác_ngộ', 'giải_thoát', '_', 'phí', 'ngồi', 'già', 'múa', 'vcl', 'rick', 'cc', 'quyền', 'dở_dở', 'giưới', 'thiệu', 'sơ', 'chút', 'mỹ_lệ', 'làm_việc', 'tại', 'hàn', 'quốc', 'nói_thẳng', 'làm_ăn', 'thua_lỗ', 'cảnh_xa', 'thương', 'cầu_thực', 'ăn_tiền', 'nhà', 'nuôi', 'con_cái', 'đủ', 'vỡ_nợ', 'lô', 'vay_mượn', 'bạn_bè', 'vỡ', 'tổng_số', 'tình_cờ', 'dám', 'vận', 'lấy', 'song', 'thủ', 'ngta', 'ân_nhân', 'cứu_mạng', 'hiện_tại', 'dư', 'sinh_sống', 'hoàn_cảnh', 'trước_kia', 'tết', 'ngon_lành', 'cảm_ơn', 'khong', 'cat', 'luon', 'mik', 'dế_choắt', 'bon', 'bệnh_hoạn', 'việt_nam', 'warriors', 'cắn', 'đít', 'sh', 'it', 'dính', 'cơn', 'chùa', 'mỡ', 'chào_đón', 'tởm', 'thể_thống', 'cống', 'rảnh', 'khen', 'thể', 'ngang', 'xàm']\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix,word_is_trained = generate_embedding(word_index,word_embedding,EMBEDDING_DIM)\n",
    "print(word_is_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjvoCKFAsKEK"
   },
   "source": [
    "# VI.Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8oYeHeBL-wqO"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        a = K.exp(ait)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return weighted_input\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[1], input_shape[2]\n",
    "\n",
    "class Addition(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Addition, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.output_dim = input_shape[-1]\n",
    "        super(Addition, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return K.sum(x, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrCQ522csUQ0"
   },
   "source": [
    "## 1.Build mode LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b-4SQdfzFQ8",
    "outputId": "d9226503-e24a-413c-993b-ffbc67f91d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.75 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 421 samples, validate on 686 samples\n",
      "Epoch 1/100\n",
      "421/421 [==============================] - 1s 2ms/step - loss: 1.6655 - acc: 0.5796 - val_loss: 1.8840 - val_acc: 0.3120\n",
      "Epoch 2/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.3493 - acc: 0.6437 - val_loss: 2.3520 - val_acc: 0.3120\n",
      "Epoch 3/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.1942 - acc: 0.6461 - val_loss: 2.1503 - val_acc: 0.3105\n",
      "Epoch 4/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.1849 - acc: 0.6366 - val_loss: 1.9698 - val_acc: 0.2857\n",
      "Epoch 5/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.1448 - acc: 0.6437 - val_loss: 2.0546 - val_acc: 0.3090\n",
      "Epoch 6/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.1008 - acc: 0.6366 - val_loss: 2.0904 - val_acc: 0.3032\n",
      "Epoch 7/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.0913 - acc: 0.6461 - val_loss: 2.0816 - val_acc: 0.2915\n",
      "Epoch 8/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.0519 - acc: 0.6461 - val_loss: 2.2290 - val_acc: 0.3076\n",
      "Epoch 9/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 1.0041 - acc: 0.6722 - val_loss: 2.1850 - val_acc: 0.2566\n",
      "Epoch 10/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.9687 - acc: 0.6675 - val_loss: 2.1719 - val_acc: 0.2755\n",
      "Epoch 11/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.9751 - acc: 0.6580 - val_loss: 2.2332 - val_acc: 0.3032\n",
      "Epoch 12/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.9210 - acc: 0.6841 - val_loss: 2.2572 - val_acc: 0.2580\n",
      "Epoch 13/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.9154 - acc: 0.6817 - val_loss: 2.2079 - val_acc: 0.2741\n",
      "Epoch 14/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.8947 - acc: 0.7007 - val_loss: 2.2746 - val_acc: 0.2930\n",
      "Epoch 15/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.8748 - acc: 0.6960 - val_loss: 2.2901 - val_acc: 0.2901\n",
      "Epoch 16/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7846 - acc: 0.7340 - val_loss: 2.2324 - val_acc: 0.2697\n",
      "Epoch 17/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.8152 - acc: 0.6983 - val_loss: 2.2891 - val_acc: 0.2799\n",
      "Epoch 18/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7782 - acc: 0.6960 - val_loss: 2.4128 - val_acc: 0.2726\n",
      "Epoch 19/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7919 - acc: 0.7078 - val_loss: 2.4130 - val_acc: 0.2653\n",
      "Epoch 20/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.8185 - acc: 0.7150 - val_loss: 2.3560 - val_acc: 0.2682\n",
      "Epoch 21/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7072 - acc: 0.7387 - val_loss: 2.3848 - val_acc: 0.2843\n",
      "Epoch 22/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7216 - acc: 0.7316 - val_loss: 2.4070 - val_acc: 0.2726\n",
      "Epoch 23/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6896 - acc: 0.7363 - val_loss: 2.4175 - val_acc: 0.2828\n",
      "Epoch 24/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.7052 - acc: 0.7648 - val_loss: 2.4026 - val_acc: 0.2813\n",
      "Epoch 25/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6552 - acc: 0.7791 - val_loss: 2.4142 - val_acc: 0.2813\n",
      "Epoch 26/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6494 - acc: 0.7743 - val_loss: 2.4719 - val_acc: 0.2988\n",
      "Epoch 27/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6329 - acc: 0.7625 - val_loss: 2.6359 - val_acc: 0.2886\n",
      "Epoch 28/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6527 - acc: 0.7648 - val_loss: 2.6179 - val_acc: 0.2915\n",
      "Epoch 29/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.6091 - acc: 0.8029 - val_loss: 2.6527 - val_acc: 0.2915\n",
      "Epoch 30/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.5254 - acc: 0.8314 - val_loss: 2.7769 - val_acc: 0.2668\n",
      "Epoch 31/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.5196 - acc: 0.8314 - val_loss: 2.8581 - val_acc: 0.2857\n",
      "Epoch 32/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.5244 - acc: 0.8266 - val_loss: 2.7667 - val_acc: 0.2828\n",
      "Epoch 33/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4831 - acc: 0.8361 - val_loss: 2.8681 - val_acc: 0.2857\n",
      "Epoch 34/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4664 - acc: 0.8266 - val_loss: 2.9829 - val_acc: 0.3061\n",
      "Epoch 35/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.5001 - acc: 0.8171 - val_loss: 2.8814 - val_acc: 0.2945\n",
      "Epoch 36/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4269 - acc: 0.8456 - val_loss: 2.9104 - val_acc: 0.3003\n",
      "Epoch 37/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.5180 - acc: 0.8266 - val_loss: 2.8868 - val_acc: 0.2959\n",
      "Epoch 38/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4264 - acc: 0.8622 - val_loss: 2.9735 - val_acc: 0.3076\n",
      "Epoch 39/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4555 - acc: 0.8314 - val_loss: 2.9965 - val_acc: 0.2930\n",
      "Epoch 40/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3899 - acc: 0.8670 - val_loss: 3.0713 - val_acc: 0.3003\n",
      "Epoch 41/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3946 - acc: 0.8646 - val_loss: 3.0953 - val_acc: 0.3047\n",
      "Epoch 42/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.4113 - acc: 0.8575 - val_loss: 3.1622 - val_acc: 0.2930\n",
      "Epoch 43/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3239 - acc: 0.8789 - val_loss: 3.2426 - val_acc: 0.3061\n",
      "Epoch 44/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3077 - acc: 0.9026 - val_loss: 3.1857 - val_acc: 0.3003\n",
      "Epoch 45/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3824 - acc: 0.8907 - val_loss: 3.2716 - val_acc: 0.3090\n",
      "Epoch 46/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3380 - acc: 0.9074 - val_loss: 3.3494 - val_acc: 0.3032\n",
      "Epoch 47/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3092 - acc: 0.8955 - val_loss: 3.3455 - val_acc: 0.2945\n",
      "Epoch 48/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3380 - acc: 0.9026 - val_loss: 3.2883 - val_acc: 0.3061\n",
      "Epoch 49/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3608 - acc: 0.8836 - val_loss: 3.2449 - val_acc: 0.2988\n",
      "Epoch 50/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3056 - acc: 0.8931 - val_loss: 3.3434 - val_acc: 0.3032\n",
      "Epoch 51/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3228 - acc: 0.8860 - val_loss: 3.4398 - val_acc: 0.3032\n",
      "Epoch 52/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.3336 - acc: 0.8836 - val_loss: 3.4188 - val_acc: 0.2988\n",
      "Epoch 53/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2271 - acc: 0.9240 - val_loss: 3.4691 - val_acc: 0.3017\n",
      "Epoch 54/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2461 - acc: 0.9050 - val_loss: 3.6388 - val_acc: 0.2959\n",
      "Epoch 55/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2412 - acc: 0.9192 - val_loss: 3.8024 - val_acc: 0.2915\n",
      "Epoch 56/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2362 - acc: 0.9192 - val_loss: 3.6729 - val_acc: 0.2784\n",
      "Epoch 57/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2435 - acc: 0.9026 - val_loss: 3.7494 - val_acc: 0.3061\n",
      "Epoch 58/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2786 - acc: 0.8955 - val_loss: 3.6417 - val_acc: 0.2915\n",
      "Epoch 59/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2013 - acc: 0.9430 - val_loss: 3.7222 - val_acc: 0.3003\n",
      "Epoch 60/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2552 - acc: 0.9145 - val_loss: 3.8588 - val_acc: 0.2799\n",
      "Epoch 61/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2260 - acc: 0.9335 - val_loss: 3.9295 - val_acc: 0.2784\n",
      "Epoch 62/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2175 - acc: 0.9169 - val_loss: 3.8560 - val_acc: 0.2741\n",
      "Epoch 63/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2615 - acc: 0.8979 - val_loss: 4.0278 - val_acc: 0.3149\n",
      "Epoch 64/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2177 - acc: 0.9145 - val_loss: 3.9984 - val_acc: 0.2872\n",
      "Epoch 65/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2318 - acc: 0.9240 - val_loss: 3.9725 - val_acc: 0.2901\n",
      "Epoch 66/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1776 - acc: 0.9287 - val_loss: 4.0229 - val_acc: 0.3105\n",
      "Epoch 67/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2216 - acc: 0.9264 - val_loss: 3.9520 - val_acc: 0.2945\n",
      "Epoch 68/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1953 - acc: 0.9192 - val_loss: 4.2063 - val_acc: 0.2945\n",
      "Epoch 69/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1948 - acc: 0.9264 - val_loss: 4.1321 - val_acc: 0.3003\n",
      "Epoch 70/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2138 - acc: 0.9359 - val_loss: 4.1237 - val_acc: 0.3017\n",
      "Epoch 71/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1582 - acc: 0.9335 - val_loss: 4.3006 - val_acc: 0.2930\n",
      "Epoch 72/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1657 - acc: 0.9525 - val_loss: 4.4142 - val_acc: 0.2886\n",
      "Epoch 73/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1868 - acc: 0.9382 - val_loss: 4.2503 - val_acc: 0.2828\n",
      "Epoch 74/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1403 - acc: 0.9501 - val_loss: 4.3893 - val_acc: 0.2930\n",
      "Epoch 75/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.2360 - acc: 0.9074 - val_loss: 4.0406 - val_acc: 0.2813\n",
      "Epoch 76/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1306 - acc: 0.9454 - val_loss: 4.0589 - val_acc: 0.2901\n",
      "Epoch 77/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1328 - acc: 0.9549 - val_loss: 4.4653 - val_acc: 0.2901\n",
      "Epoch 78/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1601 - acc: 0.9596 - val_loss: 4.4835 - val_acc: 0.2799\n",
      "Epoch 79/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1363 - acc: 0.9572 - val_loss: 4.0741 - val_acc: 0.3017\n",
      "Epoch 80/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1721 - acc: 0.9477 - val_loss: 4.2845 - val_acc: 0.3076\n",
      "Epoch 81/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1771 - acc: 0.9430 - val_loss: 4.4141 - val_acc: 0.3061\n",
      "Epoch 82/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1590 - acc: 0.9454 - val_loss: 4.3469 - val_acc: 0.2945\n",
      "Epoch 83/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1203 - acc: 0.9691 - val_loss: 4.3689 - val_acc: 0.2930\n",
      "Epoch 84/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1735 - acc: 0.9406 - val_loss: 4.1999 - val_acc: 0.3047\n",
      "Epoch 85/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9596 - val_loss: 4.2799 - val_acc: 0.3105\n",
      "Epoch 86/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1383 - acc: 0.9525 - val_loss: 4.3695 - val_acc: 0.2988\n",
      "Epoch 87/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1786 - acc: 0.9406 - val_loss: 4.4297 - val_acc: 0.2959\n",
      "Epoch 88/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1616 - acc: 0.9430 - val_loss: 4.4019 - val_acc: 0.2901\n",
      "Epoch 89/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1396 - acc: 0.9501 - val_loss: 4.3434 - val_acc: 0.2886\n",
      "Epoch 90/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1360 - acc: 0.9525 - val_loss: 4.3956 - val_acc: 0.3090\n",
      "Epoch 91/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1292 - acc: 0.9406 - val_loss: 4.3589 - val_acc: 0.3076\n",
      "Epoch 92/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1058 - acc: 0.9644 - val_loss: 4.3599 - val_acc: 0.3105\n",
      "Epoch 93/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1021 - acc: 0.9667 - val_loss: 4.3733 - val_acc: 0.3090\n",
      "Epoch 94/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1156 - acc: 0.9691 - val_loss: 4.5241 - val_acc: 0.3120\n",
      "Epoch 95/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.0950 - acc: 0.9691 - val_loss: 4.6392 - val_acc: 0.3061\n",
      "Epoch 96/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1148 - acc: 0.9596 - val_loss: 4.5899 - val_acc: 0.3032\n",
      "Epoch 97/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1197 - acc: 0.9477 - val_loss: 4.7941 - val_acc: 0.2988\n",
      "Epoch 98/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.0929 - acc: 0.9667 - val_loss: 4.7545 - val_acc: 0.2974\n",
      "Epoch 99/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1308 - acc: 0.9596 - val_loss: 4.7046 - val_acc: 0.3134\n",
      "Epoch 100/100\n",
      "421/421 [==============================] - 0s 1ms/step - loss: 0.1370 - acc: 0.9430 - val_loss: 4.6171 - val_acc: 0.2988\n"
     ]
    }
   ],
   "source": [
    "filter_nums = 256 \n",
    "def build_model():\n",
    "        inputs  = Input(shape=(maxLength, ), dtype='float64', name='inputs')    \n",
    "        embedding_layer = Embedding(input_vocab_size,EMBEDDING_DIM,weights=[embedding_matrix], input_length=maxLength, trainable=True,name = 'word_emb')(inputs)\n",
    "        embedding_layer = SpatialDropout1D(0.75)(embedding_layer)     \n",
    "        lstm_feature1 = CuDNNLSTM(filter_nums, return_sequences=True)(embedding_layer)\n",
    "        att1 = AttentionWithContext()(lstm_feature1)\n",
    "        att1 = Addition()(att1)\n",
    "        fc1 = Dropout(0.5)(Dense(256, name = 'dense_1')(att1))\n",
    "        output1 = Dense(len(classes),name=\"output1\", activation='softmax')(fc1)\n",
    "        model = Model(inputs=inputs, outputs=output1)\n",
    "        tensorBoardCallback = TensorBoard(log_dir='./logs', write_graph=True)\n",
    "        model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])   \n",
    "        history = model.fit(X_train_encode, np.array(y_train_encode), validation_data = (X_val_encode,np.array(y_val_encode)) , batch_size=50, epochs=100,callbacks=[tensorBoardCallback])\n",
    "        return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MmCoU4ksp67"
   },
   "source": [
    "## 2.Predict the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQwJ75p_WdBJ",
    "outputId": "acce5a67-e6c8-4a0e-8cac-02795280ac98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enjoyment\n"
     ]
    }
   ],
   "source": [
    "X_test_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences(X_test), maxlen=maxLength,padding=\"post\"))\n",
    "test_length = len(X_test_encode)\n",
    "\n",
    "y_predict = []\n",
    "predicted = model.predict(X_test_encode)\n",
    "for predict in predicted:\n",
    "    index2, value = max(enumerate(predict), key=operator.itemgetter(1))\n",
    "    y_predict.append(classes[index2])\n",
    "    \n",
    "print(y_predict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CWnvBNeI0joN",
    "outputId": "1a20609f-d98c-471e-8894-e3d2d6553dcb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Enjoyment', 'Other', 'Disgust', 'Sadness', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Sadness', 'Sadness', 'Disgust', 'Enjoyment', 'Disgust', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Fear', 'Surprise', 'Enjoyment', 'Enjoyment', 'Surprise', 'Sadness', 'Other', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Other', 'Other', 'Other', 'Other', 'Surprise', 'Other', 'Other', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Fear', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Anger', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Fear', 'Sadness', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Enjoyment', 'Fear', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Anger', 'Enjoyment', 'Surprise', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Anger', 'Surprise', 'Anger', 'Disgust', 'Fear', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Disgust', 'Enjoyment', 'Enjoyment', 'Sadness', 'Sadness', 'Enjoyment', 'Disgust', 'Sadness', 'Enjoyment', 'Enjoyment', 'Anger', 'Other', 'Enjoyment', 'Other', 'Other', 'Other', 'Other', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Sadness', 'Enjoyment', 'Other', 'Other', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Sadness', 'Disgust', 'Other', 'Other', 'Enjoyment', 'Sadness', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Other', 'Enjoyment', 'Disgust', 'Other', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Surprise', 'Surprise', 'Other', 'Enjoyment', 'Other', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Sadness', 'Sadness', 'Anger', 'Other', 'Enjoyment', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Disgust', 'Other', 'Other', 'Disgust', 'Other', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Anger', 'Other', 'Sadness', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Sadness', 'Surprise', 'Other', 'Enjoyment', 'Surprise', 'Enjoyment', 'Sadness', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Surprise', 'Enjoyment', 'Enjoyment', 'Other', 'Sadness', 'Other', 'Disgust', 'Sadness', 'Fear', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Sadness', 'Other', 'Sadness', 'Surprise', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Surprise', 'Surprise', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Sadness', 'Other', 'Other', 'Surprise', 'Sadness', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Sadness', 'Other', 'Other', 'Sadness', 'Other', 'Sadness', 'Enjoyment', 'Sadness', 'Fear', 'Disgust', 'Sadness', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Disgust', 'Disgust', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Anger', 'Disgust', 'Sadness', 'Enjoyment', 'Other', 'Enjoyment', 'Sadness', 'Other', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Surprise', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Sadness', 'Other', 'Other', 'Other', 'Disgust', 'Surprise', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Other', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Sadness', 'Enjoyment', 'Disgust', 'Other', 'Other', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Anger', 'Disgust', 'Other', 'Sadness', 'Enjoyment', 'Other', 'Other', 'Sadness', 'Other', 'Disgust', 'Disgust', 'Enjoyment', 'Anger', 'Sadness', 'Sadness', 'Sadness', 'Sadness', 'Disgust', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Surprise', 'Other', 'Sadness', 'Disgust', 'Sadness', 'Other', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Fear', 'Enjoyment', 'Disgust', 'Other', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Other', 'Surprise', 'Sadness', 'Surprise', 'Enjoyment', 'Sadness', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Other', 'Anger', 'Sadness', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Disgust', 'Other', 'Enjoyment', 'Disgust', 'Other', 'Enjoyment', 'Enjoyment', 'Fear', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Sadness', 'Enjoyment', 'Enjoyment', 'Disgust', 'Enjoyment', 'Enjoyment', 'Other', 'Sadness', 'Other', 'Disgust', 'Sadness', 'Other', 'Enjoyment', 'Sadness', 'Disgust', 'Other', 'Sadness', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Disgust', 'Enjoyment', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Surprise', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Disgust', 'Fear', 'Other', 'Other', 'Sadness', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Anger', 'Enjoyment', 'Other', 'Sadness', 'Surprise', 'Disgust', 'Surprise', 'Disgust', 'Enjoyment', 'Anger', 'Enjoyment', 'Disgust', 'Disgust', 'Other', 'Other', 'Other', 'Enjoyment', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Surprise', 'Enjoyment', 'Sadness', 'Other', 'Enjoyment', 'Enjoyment', 'Disgust', 'Surprise', 'Enjoyment', 'Enjoyment', 'Sadness', 'Enjoyment', 'Disgust', 'Enjoyment', 'Sadness', 'Disgust', 'Other', 'Other', 'Disgust', 'Enjoyment', 'Enjoyment', 'Fear', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Other', 'Surprise', 'Other', 'Enjoyment', 'Anger', 'Enjoyment', 'Enjoyment', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Disgust', 'Enjoyment', 'Other', 'Enjoyment', 'Disgust', 'Enjoyment', 'Enjoyment', 'Anger', 'Other', 'Enjoyment', 'Sadness', 'Enjoyment', 'Disgust', 'Sadness', 'Enjoyment', 'Surprise', 'Disgust', 'Other', 'Enjoyment', 'Other', 'Disgust', 'Other', 'Other', 'Sadness', 'Enjoyment', 'Disgust', 'Sadness', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Disgust', 'Other', 'Sadness', 'Disgust', 'Anger', 'Disgust', 'Other', 'Other', 'Enjoyment', 'Disgust', 'Sadness', 'Surprise', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Disgust', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Surprise', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Disgust', 'Sadness', 'Enjoyment', 'Enjoyment', 'Sadness', 'Surprise', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Fear', 'Enjoyment', 'Sadness', 'Other', 'Enjoyment', 'Enjoyment', 'Sadness', 'Surprise', 'Enjoyment', 'Disgust', 'Other', 'Other', 'Other', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Other', 'Other', 'Other', 'Disgust', 'Other', 'Other', 'Sadness', 'Enjoyment', 'Enjoyment', 'Enjoyment', 'Other', 'Enjoyment', 'Other', 'Other', 'Enjoyment']\n"
     ]
    }
   ],
   "source": [
    "print(y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VXZOPGcs9O_"
   },
   "source": [
    "## 3.Report the performance metrics (Accuracy, F1-score...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WoEymwgf0joR",
    "outputId": "a94d0b66-c14a-401e-d5da-a31eef2076cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result model LSTM + Attention layer\n",
      "Results of the models\n",
      "Precision:  0.27367567397178016\n",
      "Recall:  0.27705627705627706\n",
      "F1-Score:  0.27705627705627706\n",
      "Accuracy:  0.27705627705627706\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.06      0.03      0.04        40\n",
      "     Disgust       0.29      0.16      0.20       132\n",
      "   Enjoyment       0.36      0.52      0.42       193\n",
      "        Fear       0.50      0.13      0.21        46\n",
      "       Other       0.21      0.33      0.26       129\n",
      "     Sadness       0.21      0.14      0.16       116\n",
      "    Surprise       0.17      0.16      0.17        37\n",
      "\n",
      "    accuracy                           0.28       693\n",
      "   macro avg       0.26      0.21      0.21       693\n",
      "weighted avg       0.27      0.28      0.26       693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_predict, average='weighted')\n",
    "recall = recall_score(y_test, y_predict, average='weighted')\n",
    "f1score = f1_score(y_test, y_predict, average='micro')\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(\"Result model LSTM + Attention layer\")\n",
    "print(\"Results of the models\")\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1-Score: \", f1score)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2pll3z0tLDW"
   },
   "source": [
    "# VII.Enter the demo program into 1 sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3WyndWU_0joU"
   },
   "outputs": [],
   "source": [
    "def demo(str):\n",
    "  demo_pre = clean_doc(str)\n",
    "  X_demo_encode = np.array(pad_sequences(input_tokenizer.texts_to_sequences([demo_pre]), maxlen=maxLength,padding=\"post\"))\n",
    "  predicted = model.predict(X_demo_encode)\n",
    "  index2, value = max(enumerate(predicted[0]), key=operator.itemgetter(1))\n",
    "  return classes[index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dVJw8Vi5thbF",
    "outputId": "805a729d-95c1-4f7e-d1e4-332f1ed60119"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Surprise'"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo('4 : 05 ai biết info áo anh binz ko nhỉ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "1nb0I3AlEIG8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
